{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertConfig, BertForSequenceClassification\n",
    "from pytorch_pretrained_bert import *\n",
    "import torch.optim as optim\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\15049\\.pytorch_pretrained_bert\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at C:\\Users\\15049\\.pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "INFO:pytorch_pretrained_bert.modeling:extracting archive file C:\\Users\\15049\\.pytorch_pretrained_bert\\9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir C:\\Users\\15049\\AppData\\Local\\Temp\\tmptf9804r5\n",
      "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# bert_model = BertModel.from_pretrained('bert-base-uncased') \n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# modelConfig = BertConfig.from_pretrained('bert_config')\n",
    "\n",
    "data_path='./dataset/' # Folders for training and test sets 训练集、测试集存放目录 \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True, do_basic_tokenize=True)# Get the word list 获取词表\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels = 3)# Load the BERT pre-trained model 获取BERT预训练模型文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file to load the labels of test set 读取CSV文件，用于获取测试集labels\n",
    "def read_CSV(file_name):\n",
    "    testset_features=[] #features' list\n",
    "    testset_labels=[]   #labels' list\n",
    "    try:\n",
    "            with open(file_name,'r') as file_object:                \n",
    "                    reader_CSV=csv.reader(file_object)\n",
    "                    num_lines = 0\n",
    "                    for oneline in reader_CSV:\n",
    "                            testset_features.append(oneline[0])\n",
    "                            testset_labels.append(oneline[1])\n",
    "                            num_lines += 1\n",
    "    except FileNotFoundError:\n",
    "            print('Cannot find the .csv file ' + file_name+'.') \n",
    "    else:\n",
    "            print('The test set index list is : \\n\\b'+str(testset_features)+'\\n\\n\\n\\nThe test set lable list is: \\n\\b'+str(testset_labels))\n",
    "            print('\\n\\nSucceed to load the test set labels from '+file_name+'! The data set contains '+str(num_lines)+' samples in total.')    \n",
    "            return testset_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence processing function  语句处理函数定义\n",
    "def preprocessing (filename):\n",
    "\n",
    "    df = pd.read_csv(os.path.join(data_path,filename), delimiter='\\t', header=None,\\\n",
    "         names=['id', 'tweet', 'subtask_a', 'subtask_b', 'subtask_c'])  # names=['sentence_source', 'label', 'label_notes', 'sentence']\n",
    "\n",
    "    # Extract sentences 提取语句\n",
    "    sentencses=['[CLS] ' + sent + ' [SEP]' for sent in df.tweet.values][1:]  \n",
    "    labels=df.subtask_c.values[1:]          # define point\n",
    "\n",
    "    print(\"第一句话:\",sentencses[0])\n",
    "    tokenized_sents=[tokenizer.tokenize(sent) for sent in sentencses]\n",
    "    print(\"tokenized的第一句话:\",tokenized_sents[0])\n",
    "\n",
    "    # Define the max length of sentences 定义句子最大长度\n",
    "    MAX_LEN=128\n",
    "\n",
    "    # Transfer the segmented sentences into numbers 将分割后的句子转化成数字  word-->idx\n",
    "    input_ids=[tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_sents]\n",
    "    print(\"转化后的第一个句子:\",input_ids[0])\n",
    "\n",
    "    # Use keras for PADDING, runcating means truncation greater than the maximum length, truncation greater than 128, PADDING less than 128 用keras做PADDING，runcating表示大于最大长度截断，大于128做截断，小于128做PADDING\n",
    "    input_ids = keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"int64\", truncating=\"post\", padding=\"post\")\n",
    "    print(\"Padding 第一个句子:\",input_ids[0])\n",
    "\n",
    "\n",
    "    # Generate mask 生成mask\n",
    "    attention_masks = []\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "    print(\"第一个attention mask:\",attention_masks[0])\n",
    "    return input_ids,attention_masks,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一句话: [CLS] @USER @USER Go home you’re drunk!!! @USER #MAGA #Trump2020 👊🇺🇸👊 URL [SEP]\n",
      "tokenized的第一句话: ['[CLS]', '@', 'user', '@', 'user', 'go', 'home', 'you', '’', 're', 'drunk', '!', '!', '!', '@', 'user', '#', 'mag', '##a', '#', 'trump', '##20', '##20', '[UNK]', 'ur', '##l', '[SEP]']\n",
      "转化后的第一个句子: [101, 1030, 5310, 1030, 5310, 2175, 2188, 2017, 1521, 2128, 7144, 999, 999, 999, 1030, 5310, 1001, 23848, 2050, 1001, 8398, 11387, 11387, 100, 24471, 2140, 102]\n",
      "Padding 第一个句子: [  101  1030  5310  1030  5310  2175  2188  2017  1521  2128  7144   999\n",
      "   999   999  1030  5310  1001 23848  2050  1001  8398 11387 11387   100\n",
      " 24471  2140   102     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "第一个attention mask: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "第一句话: [CLS] #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA Democrats Support Antifa, Muslim Brotherhood, MS13, ISIS, Pedophilia, Child Trafficking, Taxpayer Funded Abortion’s, Election Fraud, Sedition And Treason!!! #LockThemAllUp #WWG1WGA #QAnon ⁦@USER URL [SEP]\n",
      "tokenized的第一句话: ['[CLS]', '#', 'who', '##is', '##q', '#', 'where', '##st', '##hes', '##er', '##ver', '#', 'dump', '##nik', '##e', '#', 'dec', '##las', '##fi', '##sa', 'democrats', 'support', 'anti', '##fa', ',', 'muslim', 'brotherhood', ',', 'ms', '##13', ',', 'isis', ',', 'pe', '##do', '##phi', '##lia', ',', 'child', 'trafficking', ',', 'taxpayer', 'funded', 'abortion', '’', 's', ',', 'election', 'fraud', ',', 'se', '##dition', 'and', 'treason', '!', '!', '!', '#', 'lock', '##the', '##mal', '##lu', '##p', '#', 'w', '##wg', '##1', '##wg', '##a', '#', 'q', '##ano', '##n', '@', 'user', 'ur', '##l', '[SEP]']\n",
      "转化后的第一个句子: [101, 1001, 2040, 2483, 4160, 1001, 2073, 3367, 15689, 2121, 6299, 1001, 15653, 8238, 2063, 1001, 11703, 8523, 8873, 3736, 8037, 2490, 3424, 7011, 1010, 5152, 12865, 1010, 5796, 17134, 1010, 18301, 1010, 21877, 3527, 21850, 6632, 1010, 2775, 11626, 1010, 26980, 6787, 11324, 1521, 1055, 1010, 2602, 9861, 1010, 7367, 20562, 1998, 14712, 999, 999, 999, 1001, 5843, 10760, 9067, 7630, 2361, 1001, 1059, 27767, 2487, 27767, 2050, 1001, 1053, 6761, 2078, 1030, 5310, 24471, 2140, 102]\n",
      "Padding 第一个句子: [  101  1001  2040  2483  4160  1001  2073  3367 15689  2121  6299  1001\n",
      " 15653  8238  2063  1001 11703  8523  8873  3736  8037  2490  3424  7011\n",
      "  1010  5152 12865  1010  5796 17134  1010 18301  1010 21877  3527 21850\n",
      "  6632  1010  2775 11626  1010 26980  6787 11324  1521  1055  1010  2602\n",
      "  9861  1010  7367 20562  1998 14712   999   999   999  1001  5843 10760\n",
      "  9067  7630  2361  1001  1059 27767  2487 27767  2050  1001  1053  6761\n",
      "  2078  1030  5310 24471  2140   102     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "第一个attention mask: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "The test set index list is : \n",
      "\b['15923', '60133', '83681', '65507', '34263', '49139', '58995', '88490', '46444', '60587', '44546', '51628', '40110', '15998', '96457', '70841', '46139', '40386', '98916', '32190', '27550', '24040', '73516', '88905', '42112', '37740', '72405', '47445', '27158', '17183', '42325', '15180', '58026', '81352', '33133', '91036', '22882', '40842', '73612', '15815', '24049', '79204', '46229', '34669', '34575', '20357', '52445', '10991', '57135', '72369', '42192', '43587', '41553', '62986', '70051', '73642', '11645', '72274', '21524', '84579', '59519', '51386', '10918', '57541', '25125', '61295', '53351', '25463', '85613', '85886', '54621', '80397', '67926', '41588', '51948', '28196', '50376', '45304', '27228', '57326', '88221', '97610', '63048', '30075', '60466', '25685', '16333', '78417', '51525', '84550', '23530', '57284', '59700', '13433', '80197', '79309', '47696', '55633', '87934', '27455', '83466', '48657', '86611', '12193', '63985', '20522', '70324', '89329', '11100', '83240', '81890', '72037', '94607', '72531', '98575', '58342', '39429', '18786', '67049', '98216', '37649', '59751', '92215', '22311', '75125', '21354', '22594', '33394', '56513', '59691', '41997', '56637', '97410', '77746', '17704', '61674', '39860', '79934', '55048', '76833', '55832', '32056', '67841', '94427', '96594', '62689', '35968', '14516', '15866', '57185', '91430', '29203', '27913', '46410', '16856', '89200', '45269', '15437', '91472', '98531', '98685', '31665', '38732', '46983', '22067', '79756', '99563', '23542', '83416', '67418', '58632', '35612', '96905', '79778', '90328', '71350', '43782', '58287', '32061', '63129', '74797', '25177', '87428', '39400', '88745', '70443', '90327', '65545', '58543', '79222', '72401', '31354', '29008', '41590', '72523', '14640', '74909', '96397', '34030', '29113', '11286', '41821', '76379', '52080', '51762', '71592', '78688', '76135', '30778', '22569', '48938', '41438', '73439']\n",
      "\n",
      "\n",
      "\n",
      "The test set lable list is: \n",
      "\b['OTH', 'GRP', 'IND', 'OTH', 'IND', 'IND', 'GRP', 'IND', 'IND', 'IND', 'GRP', 'IND', 'GRP', 'OTH', 'GRP', 'IND', 'GRP', 'GRP', 'OTH', 'OTH', 'IND', 'GRP', 'IND', 'IND', 'GRP', 'IND', 'IND', 'OTH', 'OTH', 'IND', 'OTH', 'IND', 'IND', 'IND', 'IND', 'GRP', 'IND', 'IND', 'IND', 'GRP', 'IND', 'GRP', 'GRP', 'GRP', 'GRP', 'IND', 'IND', 'IND', 'GRP', 'IND', 'GRP', 'IND', 'GRP', 'OTH', 'GRP', 'IND', 'GRP', 'OTH', 'IND', 'GRP', 'GRP', 'IND', 'OTH', 'IND', 'IND', 'OTH', 'IND', 'GRP', 'GRP', 'OTH', 'IND', 'GRP', 'GRP', 'GRP', 'OTH', 'IND', 'OTH', 'IND', 'GRP', 'OTH', 'GRP', 'OTH', 'IND', 'IND', 'GRP', 'IND', 'GRP', 'IND', 'IND', 'GRP', 'IND', 'GRP', 'IND', 'GRP', 'IND', 'GRP', 'GRP', 'IND', 'GRP', 'GRP', 'IND', 'GRP', 'GRP', 'GRP', 'IND', 'IND', 'GRP', 'OTH', 'OTH', 'GRP', 'IND', 'IND', 'GRP', 'OTH', 'IND', 'IND', 'IND', 'OTH', 'IND', 'OTH', 'IND', 'IND', 'GRP', 'GRP', 'OTH', 'IND', 'GRP', 'IND', 'OTH', 'GRP', 'IND', 'IND', 'IND', 'IND', 'GRP', 'GRP', 'IND', 'IND', 'OTH', 'OTH', 'GRP', 'OTH', 'IND', 'OTH', 'GRP', 'IND', 'GRP', 'GRP', 'IND', 'GRP', 'OTH', 'IND', 'IND', 'IND', 'GRP', 'IND', 'GRP', 'GRP', 'IND', 'GRP', 'GRP', 'GRP', 'GRP', 'OTH', 'IND', 'GRP', 'IND', 'GRP', 'OTH', 'IND', 'IND', 'IND', 'IND', 'IND', 'IND', 'IND', 'IND', 'IND', 'IND', 'IND', 'IND', 'OTH', 'IND', 'GRP', 'IND', 'GRP', 'IND', 'GRP', 'IND', 'GRP', 'IND', 'OTH', 'OTH', 'IND', 'GRP', 'OTH', 'IND', 'GRP', 'GRP', 'GRP', 'IND', 'IND', 'GRP', 'GRP', 'IND', 'IND', 'GRP', 'IND', 'IND', 'GRP', 'IND', 'GRP', 'GRP']\n",
      "\n",
      "\n",
      "Succeed to load the test set labels from .\\dataset\\labels-levelc.csv! The data set contains 213 samples in total.\n",
      "[0, 1, 2, 0, 0, 1, 0, 2, 2, 0, 2, 0, 2, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 0, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 2, 1, 2, 0, 0, 2, 0, 0, 2, 1, 2, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 2, 2, 2, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 0, 2, 0, 2, 2, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 1, 0, 0, 2, 0, 0, 2, 0, 2, 0, 2, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 2, 0, 2, 1, 0, 2, 0, 1, 1, 2, 2, 0, 2, 0, 1, 2, 0, 2, 0, 2, 0, 1, 1, 1, 0, 0, 2, 0, 0, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 1, 2, 0, 0, 0, 2, 0, 1, 2, 2, 0, 2, 1, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 2, 0, 0, 2, 2, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 1, 0, 2, 0, 1, 2, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 2, 0, 2, 0, 1, 2, 2, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 2, 0, 2, 2, 0, 0, 2, 0, 2, 2, 1, 2, 0, 0, 0, 2, 0, 2, 0, 2, 0, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 1, 2, 2, 1, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 2, 0, 2, 0, 1, 0, 0, 1, 0, 0, 1, 2, 0, 2, 1, 2, 0, 1, 0, 0, 1, 0, 0, 2, 1, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 1, 2, 0, 2, 0, 0, 0, 2, 2, 0, 1, 2, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 2, 2, 0, 1, 2, 0, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 2, 0, 1, 2, 1, 2, 0, 0, 2, 0, 2, 0, 2, 2, 0, 1, 2, 0, 2, 0, 0, 0, 2, 0, 2, 2, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 2, 0, 1, 0, 0, 1, 0, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 2, 1, 2, 0, 0, 1, 1, 2, 0, 0, 2, 2, 1, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 1, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 2, 2, 2, 2, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0, 2, 1, 0, 2, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 2, 2, 0, 1, 0, 0, 1, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 0, 0, 2, 1, 2, 1, 0, 0, 1, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 1, 0, 0, 2, 2, 0, 0, 0, 2, 0, 1, 0, 0, 1, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 0, 1, 0, 2, 0, 2, 0, 0, 1, 2, 0, 0, 2, 0, 0, 0, 2, 0, 1, 1, 0, 2, 2, 0, 0, 2, 0, 0, 0, 2, 2, 0, 2, 1, 0, 0, 2, 1, 0, 0, 0, 0, 2, 1, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 2, 2, 0, 0, 0, 1, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 0, 0, 1, 0, 2, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 1, 0, 2, 0, 0, 1, 2, 1, 1, 2, 0, 1, 2, 2, 1, 1, 2, 2, 0, 2, 1, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 1, 2, 0, 2, 0, 2, 0, 0, 0, 0, 2, 2, 2, 0, 2, 0, 1, 0, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 2, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 2, 0, 1, 2, 0, 2, 2, 0, 0, 2, 0, 2, 2, 2, 0, 0, 1, 2, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 1, 2, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 1, 2, 2, 0, 2, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 2, 2, 0, 2, 2, 2, 0, 0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 2, 0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 2, 1, 0, 0, 2, 2, 1, 0, 0, 0, 2, 2, 0, 0, 2, 2, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 0, 1, 2, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 0, 2, 1, 1, 0, 0, 0, 1, 2, 0, 1, 0, 2, 0, 2, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 0, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 2, 1, 0, 0, 2, 2, 2, 1, 1, 0, 0, 0, 2, 2, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 2, 2, 0, 0, 2, 2, 2, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 1, 0, 2, 1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 2, 0, 0, 0, 1, 2, 2, 0, 2, 2, 0, 2, 0, 0, 0, 2, 2, 2, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 1, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 1, 2, 0, 0, 0, 1, 2, 0, 0, 0, 2, 0, 1, 0, 2, 1, 0, 1, 2, 0, 2, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 2, 1, 0, 1, 1, 0, 0, 0, 2, 0, 0, 2, 0, 1, 0, 0, 1, 0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 2, 1, 0, 0, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 2, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 1, 0, 0, 2, 2, 2, 1, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 2, 1, 2, 2, 0, 1, 2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 2, 0, 1, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 2, 0, 2, 0, 0, 0, 2, 0, 2, 0, 2, 1, 0, 2, 0, 0, 0, 0, 2, 1, 0, 2, 0, 2, 0, 2, 2, 1, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 2, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 0, 2, 1, 2, 0, 2, 0, 2, 0, 0, 0, 2, 2, 0, 1, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 2, 2, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 1, 0, 0, 2, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 2, 1, 0, 0, 0, 2, 2, 0, 0, 2, 0, 2, 2, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 2, 2, 1, 0, 0, 2, 2, 2, 1, 0, 2, 2, 0, 0, 1, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, 1, 0, 2, 1, 0, 0, 2, 2, 2, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 2, 2, 0, 0, 1, 1, 2, 1, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 1, 0, 2, 0, 0, 2, 0, 0, 2, 0, 2, 1, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 2, 0, 0, 0, 1, 1, 0, 2, 2, 0, 0, 1, 0, 0, 1, 0, 0, 0, 2, 2, 2, 0, 0, 2, 0, 0, 1, 2, 2, 1, 0, 1, 1, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 2, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 2, 2, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 2, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 1, 0, 2, 1, 0, 0, 2, 0, 0, 0, 2, 0, 1, 0, 2, 0, 2, 2, 0, 0, 1, 0, 0, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 2, 2, 1, 1, 2, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 1, 0, 0, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 1, 2, 2, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 2, 0, 2, 0, 1, 2, 2, 0, 1, 2, 0, 0, 1, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 2, 1, 1, 2, 1, 0, 1, 2, 1, 2, 0, 0, 1, 2, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 1, 0, 2, 0, 0, 2, 1, 0, 0, 2, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 2, 0, 0, 2, 0, 0, 0, 0, 2, 0, 2, 2, 2, 1, 0, 0, 0, 1, 0, 0, 0, 2, 2, 0, 0, 2, 0, 0, 2, 2, 2, 1, 2, 2, 0, 2, 2, 0, 2, 0, 0, 1, 0, 2, 2, 0, 0, 2, 0, 0, 0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 1, 0, 2, 0, 2, 2, 0, 2, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 0, 2, 0, 1, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 1, 2, 2, 0, 2, 0, 2, 0, 0, 1, 1, 2, 0, 2, 0, 0, 2, 2, 1, 0, 0, 1, 0, 2, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 2, 0, 1, 2, 0, 1, 2, 2, 2, 0, 0, 0, 1, 2, 0, 2, 0, 0, 1, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 1, 2, 2, 0, 0, 2, 0, 0, 0, 0, 2, 0, 1, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 1, 0, 2, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 2, 0, 2, 0, 1, 2, 1, 2, 0, 2, 0, 2, 2, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 2, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, 1, 0, 2, 2, 2, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 2, 2, 0, 0, 0, 2, 1, 1, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 2, 1, 0, 0, 2, 1, 1, 2, 2, 0, 2, 0, 2, 2, 0, 1, 0, 1, 2, 0, 2, 0, 0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 1, 0, 0, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 2, 1, 0, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 1, 0, 0, 1, 0, 2, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 2, 2, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 0, 2, 0, 1, 2, 0, 2, 0, 2, 0, 1, 0, 0, 0, 2, 2, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 2, 0, 1, 0, 2, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 2, 0, 2, 0, 1, 2, 0, 0, 2, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 1, 2, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 1, 0, 2, 2, 0, 1]\n",
      "[1, 2, 0, 1, 0, 0, 2, 0, 0, 0, 2, 0, 2, 1, 2, 0, 2, 2, 1, 1, 0, 2, 0, 0, 2, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 2, 2, 2, 2, 0, 0, 0, 2, 0, 2, 0, 2, 1, 2, 0, 2, 1, 0, 2, 2, 0, 1, 0, 0, 1, 0, 2, 2, 1, 0, 2, 2, 2, 1, 0, 1, 0, 2, 1, 2, 1, 0, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2, 0, 0, 2, 1, 1, 2, 0, 0, 2, 1, 0, 0, 0, 1, 0, 1, 0, 0, 2, 2, 1, 0, 2, 0, 1, 2, 0, 0, 0, 0, 2, 2, 0, 0, 1, 1, 2, 1, 0, 1, 2, 0, 2, 2, 0, 2, 1, 0, 0, 0, 2, 0, 2, 2, 0, 2, 2, 2, 2, 1, 0, 2, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0, 2, 0, 2, 0, 2, 0, 1, 1, 0, 2, 1, 0, 2, 2, 2, 0, 0, 2, 2, 0, 0, 2, 0, 0, 2, 0, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# Read the file for training set and test set 读取训练集，测试集文件\n",
    "[train_inputs,train_masks,train_labels] = preprocessing (\"Qc_olid-training-v1.0.tsv\")# define point\n",
    "[test_inputs,test_masks,_] = preprocessing (\"testset-levelc.tsv\")   # define point\n",
    "test_labels = read_CSV(file_name=\".\\dataset\\labels-levelc.csv\")     # define point\n",
    "\n",
    "\n",
    "train_labels = [0 if i=='IND' else i for i in train_labels]         # define point\n",
    "train_labels = [1 if i=='OTH' else i for i in train_labels]         # define point\n",
    "train_labels = [2 if i=='GRP' else i for i in train_labels]         # define point\n",
    "\n",
    "test_labels = [0 if i=='IND' else i for i in test_labels]           # define point\n",
    "test_labels = [1 if i=='OTH' else i for i in test_labels]           # define point\n",
    "test_labels = [2 if i=='GRP' else i for i in test_labels]           # define point\n",
    "\n",
    "print(train_labels)\n",
    "print(test_labels)                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the training and validation sets into tensors, and acquire inputs, masks, and labels 将训练集、验证集转化成tensor，获取inputs，masks和labels\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "test_masks = torch.tensor(test_masks)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "\n",
    "# Create dataloader 创建dataloader\n",
    "batch_size = 16\n",
    "train_data = torch.utils.data.TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = torch.utils.data.RandomSampler(train_data)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "test_data = torch.utils.data.TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = torch.utils.data.SequentialSampler(test_data)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): BertLayerNorm()\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): BertLayerNorm()\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): BertLayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.cuda())\n",
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer definition, try Adam and SDG with momentum 优化器定义，尝试Adam和带momentum的的SDG\n",
    "param_optimizer = list(model.named_parameters())\n",
    "\n",
    "# optimizer = BertAdam(optimizer_grouped_parameters, lr=10**-4, warmup=-1, t_total=-1)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.00012, weight_decay= 10**-7, momentum = 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function for visualisation of loss and accuracy  loss和accuracy的可视化函数定义\n",
    "def convergence_result_visualisation (average_loss_epoch, accuracy_testset, list_epochs):\n",
    "  plt.figure(figsize=(8,4))\n",
    "  plt.subplot(1,2,1)\n",
    "  plt.title(\"Accuracy on test set with Epochs\")\n",
    "  plt.xlabel(\"Training Epochs\")\n",
    "  plt.ylabel(\"Accuracy on test set\")\n",
    "  plt.plot(list_epochs, accuracy_testset ,label=\"Accuracy\")\n",
    "  plt.ylim((0,1.))\n",
    "  plt.xlim(left=0)\n",
    "  plt.legend(frameon=True)\n",
    "\n",
    "  plt.subplot(1,2,2)\n",
    "  plt.title(\"Loss function value on training set with Epochs\")\n",
    "  plt.xlabel(\"Training Epochs\")\n",
    "  plt.ylabel(\"Loss function value on training set\")\n",
    "  plt.plot(list_epochs,average_loss_epoch, label=\"Loss\",color='red')\n",
    "  plt.xlim(left=0)\n",
    "  plt.legend(frameon=True)\n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|███████▌                                                                    | 1/10 [01:23<12:27, 83.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss function on training set: 0.8857119367937002\n",
      "Accuracy on test set: 0.4660714285714286\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|███████████████▏                                                            | 2/10 [02:47<11:06, 83.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss function on training set: 0.8300133997520792\n",
      "Accuracy on test set: 0.6178571428571429\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|██████████████████████▊                                                     | 3/10 [04:10<09:44, 83.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss function on training set: 0.7702545607040939\n",
      "Accuracy on test set: 0.6767857142857142\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|██████████████████████████████▍                                             | 4/10 [05:35<08:22, 83.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss function on training set: 0.7156551453311748\n",
      "Accuracy on test set: 0.6901785714285714\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|██████████████████████████████████████                                      | 5/10 [06:59<06:59, 83.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss function on training set: 0.6877037368929435\n",
      "Accuracy on test set: 0.7125\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|█████████████████████████████████████████████▌                              | 6/10 [08:23<05:36, 84.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss function on training set: 0.6741135016145039\n",
      "Accuracy on test set: 0.7035714285714285\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|█████████████████████████████████████████████████████▏                      | 7/10 [09:48<04:12, 84.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss function on training set: 0.663501595457395\n",
      "Accuracy on test set: 0.7080357142857142\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████████████████████████████████████████████████████████▊               | 8/10 [11:11<02:47, 83.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss function on training set: 0.6551864768987821\n",
      "Accuracy on test set: 0.7080357142857142\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|████████████████████████████████████████████████████████████████████▍       | 9/10 [12:34<01:23, 83.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss function on training set: 0.6499652023055426\n",
      "Accuracy on test set: 0.6946428571428571\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|███████████████████████████████████████████████████████████████████████████| 10/10 [13:57<00:00, 83.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss function on training set: 0.6401664996711315\n",
      "Accuracy on test set: 0.7214285714285714\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the class corresponding to maximum probability with argmax and then calculate the accuracy  用argmax获取最大概率对应类，计算准确率\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Begin training 开始训练\n",
    "train_loss_set = []\n",
    "epochs = 10\n",
    "plot_accuracy=[]\n",
    "plot_loss=[]\n",
    "for num_epochs in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to('cuda') for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        #取第一个位置，BertForSequenceClassification第一个位置是Loss，第二个位置是[CLS]的logits\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html \n",
    "        # https://github.com/yuzcccc/pytorch-pretrained-BERT/blob/master/examples/run_classifier.py\n",
    "        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        train_loss_set.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "    \n",
    "    # Performance evaluation 模型performance评估\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to('cuda') for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "    print(\"\\nLoss function on training set: {}\".format(tr_loss / nb_tr_steps))\n",
    "    print(\"Accuracy on test set: {}\".format(eval_accuracy / nb_eval_steps))\n",
    "    print('\\n')\n",
    "    plot_loss.append(tr_loss / nb_tr_steps)    \n",
    "    plot_accuracy.append(eval_accuracy / nb_eval_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_axis = list(range(1,num_epochs+2))\n",
    "convergence_result_visualisation (plot_loss, plot_accuracy, x_axis) # Visualise network's performance 绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
